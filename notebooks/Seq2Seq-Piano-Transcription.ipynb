{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pioneerdj/anaconda3/envs/torch_music/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.13.1+cu116\n",
      "Torchaudio: 0.13.1+cu116\n",
      "PyTorch Lightning: 1.9.4\n",
      "Transformers: 4.29.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torchaudio\n",
    "\n",
    "import tqdm\n",
    "import pretty_midi as pm\n",
    "import mir_eval\n",
    "import transformers\n",
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mir_eval.sonify as sonify\n",
    "import mir_eval.display as display\n",
    "\n",
    "\n",
    "\n",
    "SR = 16000\n",
    "AUDIO_SEGMENT_SEC = 2.0\n",
    "SEGMENT_N_FRAMES = 200\n",
    "FRAME_STEP_SIZE_SEC = 0.01\n",
    "FRAME_PER_SEC = 100 \n",
    "\n",
    "\n",
    "pl.seed_everything(1234)\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Torchaudio: {torchaudio.__version__}')\n",
    "print(f'PyTorch Lightning: {pl.__version__}')\n",
    "print(f'Transformers: {transformers.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-like token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIDI-like token definitions\n",
    "\n",
    "N_NOTE = 128\n",
    "N_TIME = 205\n",
    "N_SPECIAL = 3\n",
    "\n",
    "voc_single_track = {\n",
    "    \"pad\": 0,\n",
    "    \"eos\": 1,\n",
    "    \"endtie\": 2,\n",
    "    \"note\": N_SPECIAL,\n",
    "    \"onset\": N_SPECIAL+N_NOTE,\n",
    "    \"time\": N_SPECIAL+N_NOTE+2,\n",
    "    \"n_voc\": N_SPECIAL+N_NOTE+2+N_TIME+3,\n",
    "    \"keylist\": [\"pad\", \"eos\", \"endtie\", \"note\", \"onset\", \"time\"]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Given a MIDI file, the notes are first extended according to its pedal signal (`pm_apply_pedal()`). Then the notes are splitted into segments and transformed into token lists (`get_segment_tokens()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Event:\n",
    "    prog: int\n",
    "    onset: bool\n",
    "    pitch: int\n",
    "\n",
    "class MIDITokenExtractor:\n",
    "    def __init__(self, midi_path, voc_dict, apply_pedal=True):\n",
    "        self.pm = pm.PrettyMIDI(midi_path)\n",
    "        if apply_pedal:\n",
    "            self.pm_apply_pedal(self.pm)\n",
    "        self.voc_dict = voc_dict\n",
    "        self.multi_track = \"instrument\" in voc_dict\n",
    "\n",
    "    def pm_apply_pedal(self, pm: pm.PrettyMIDI, program=0):\n",
    "        \"\"\"\n",
    "        Apply sustain pedal by stretching the notes in the pm object.\n",
    "        \"\"\"\n",
    "        # 1: Record the onset positions of each notes as a dictionary\n",
    "        onset_dict = dict()     \n",
    "        for note in pm.instruments[program].notes:\n",
    "            if note.pitch in onset_dict:\n",
    "                onset_dict[note.pitch].append(note.start)\n",
    "            else:\n",
    "                onset_dict[note.pitch] = [note.start]\n",
    "        for k in onset_dict.keys():\n",
    "            onset_dict[k] = np.sort(onset_dict[k])\n",
    "            \n",
    "        # 2: Record the pedal on/off state of each time frame\n",
    "        arr_pedal = np.zeros(\n",
    "            round(pm.get_end_time()*FRAME_PER_SEC)+100, dtype=bool)\n",
    "        pedal_on_time = -1\n",
    "        list_pedaloff_time = []\n",
    "        for cc in pm.instruments[program].control_changes:\n",
    "            if cc.number == 64:\n",
    "                if (cc.value > 0) and (pedal_on_time < 0):\n",
    "                    pedal_on_time = round(cc.time*FRAME_PER_SEC)\n",
    "                elif (cc.value == 0) and (pedal_on_time >= 0):\n",
    "                    pedal_off_time = round(cc.time*FRAME_PER_SEC)\n",
    "                    arr_pedal[pedal_on_time:pedal_off_time] = True\n",
    "                    list_pedaloff_time.append(cc.time)\n",
    "                    pedal_on_time = -1\n",
    "        list_pedaloff_time = np.sort(list_pedaloff_time)\n",
    "        \n",
    "        # 3: Stretch the notes (modify note.end)\n",
    "        for note in pm.instruments[program].notes:\n",
    "            # 3-1: Determine whether sustain pedal is on at note.end. If not, do nothing.\n",
    "            # 3-2: Find the next note onset time and next pedal off time after note.end.\n",
    "            # 3-3: Extend note.end till the minimum of next_onset and next_pedaloff.\n",
    "            note_off_frame = round(note.end*FRAME_PER_SEC)\n",
    "            pitch = note.pitch\n",
    "            if arr_pedal[note_off_frame]:\n",
    "                next_onset = np.argwhere(onset_dict[pitch] > note.end)\n",
    "                next_onset = np.inf if len(\n",
    "                    next_onset) == 0 else onset_dict[pitch][next_onset[0, 0]]\n",
    "                next_pedaloff = np.argwhere(list_pedaloff_time > note.end)\n",
    "                next_pedaloff = np.inf if len(\n",
    "                    next_pedaloff) == 0 else list_pedaloff_time[next_pedaloff[0, 0]]\n",
    "                new_noteoff_time = max(note.end, min(next_onset, next_pedaloff))\n",
    "                new_noteoff_time = min(new_noteoff_time, pm.get_end_time())\n",
    "                note.end = new_noteoff_time\n",
    "    \n",
    "    def get_segment_tokens(self, start, end):\n",
    "        \"\"\"\n",
    "        Transform a segment of the MIDI file into a sequence of tokens.\n",
    "        \"\"\"\n",
    "        dict_event = dict() # a dictionary that maps time to a list of events.\n",
    "\n",
    "        def append_to_dict_event(time, item):\n",
    "            if time in dict_event:\n",
    "                dict_event[time].append(item)\n",
    "            else:\n",
    "                dict_event[time] = [item]\n",
    "\n",
    "        list_events = []        # events section\n",
    "        list_tie_section = []   # tie section\n",
    "\n",
    "        for instrument in self.pm.instruments:\n",
    "            prog = instrument.program\n",
    "            for note in instrument.notes:\n",
    "                note_end = round(note.end * FRAME_PER_SEC)\n",
    "                note_start = round(note.start * FRAME_PER_SEC)\n",
    "                if (note_end < start) or (note_start >= end):\n",
    "                    continue\n",
    "                if (note_start < start) and (note_end >= start):\n",
    "                    # If the note starts before the segment, but ends in the segment\n",
    "                    # it is added to the tie section.\n",
    "                    list_tie_section.append(self.voc_dict[\"note\"] + note.pitch)\n",
    "                    if note_end < end:\n",
    "                        append_to_dict_event(\n",
    "                            note_end - start, Event(prog, False, note.pitch)\n",
    "                        )\n",
    "                    continue\n",
    "                assert note_start >= start\n",
    "                append_to_dict_event(note_start - start, Event(prog, True, note.pitch))\n",
    "                if note_end < end:\n",
    "                    append_to_dict_event(\n",
    "                        note_end - start, Event(prog, False, note.pitch)\n",
    "                    )\n",
    "\n",
    "        cur_onset = None\n",
    "        cur_prog = -1\n",
    "        for time in sorted(dict_event.keys()):\n",
    "            list_events.append(self.voc_dict[\"time\"] + time)\n",
    "            for event in sorted(dict_event[time], key=attrgetter(\"pitch\", \"onset\")):\n",
    "                if cur_onset != event.onset:\n",
    "                    cur_onset = event.onset\n",
    "                    list_events.append(self.voc_dict[\"onset\"] + int(event.onset))\n",
    "                list_events.append(self.voc_dict[\"note\"] + event.pitch)\n",
    "                \n",
    "        # Concatenate tie section, endtie token, and event section\n",
    "        list_tie_section.append(self.voc_dict[\"endtie\"])\n",
    "        list_events.append(self.voc_dict[\"eos\"])\n",
    "        tokens = np.concatenate((list_tie_section, list_events)).astype(int)\n",
    "        return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detokenizer\n",
    "Transforms a list of MIDI-like token sequences into a MIDI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_id(voc_dict: dict, id: int):\n",
    "    keys = voc_dict[\"keylist\"]\n",
    "    # anchors = [voc_dict[k] for k in keys]\n",
    "    token_name = keys[0]\n",
    "    for k in keys:\n",
    "        if id < voc_dict[k]:\n",
    "            break\n",
    "        token_name = k\n",
    "    token_id = id - voc_dict[token_name]\n",
    "    return token_name, token_id\n",
    "\n",
    "\n",
    "def to_second(n):\n",
    "    return n * FRAME_STEP_SIZE_SEC\n",
    "\n",
    "\n",
    "def find_note(list, n):\n",
    "    li_elem = [a for a, _ in list]\n",
    "    try:\n",
    "        idx = li_elem.index(n)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "    return idx\n",
    "\n",
    "\n",
    "def token_seg_list_to_midi(token_seg_list: list):\n",
    "    \"\"\"\n",
    "    Transform a list of token sequences into a MIDI file.\n",
    "    \"\"\"\n",
    "    midi_data = pm.PrettyMIDI()\n",
    "    piano_program = pm.instrument_name_to_program(\"Acoustic Grand Piano\")\n",
    "    piano = pm.Instrument(program=piano_program)\n",
    "    list_onset = []\n",
    "    cur_time = 0\n",
    "    for token_seg in token_seg_list:\n",
    "        list_tie = []\n",
    "        cur_relative_time = -1\n",
    "        cur_onset = -1\n",
    "        tie_end = False\n",
    "        for token in token_seg:\n",
    "            token_name, token_id = parse_id(voc_single_track, token)\n",
    "            if token_name == \"note\":\n",
    "                if not tie_end:\n",
    "                    list_tie.append(token_id)\n",
    "                elif cur_onset == 1:\n",
    "                    list_onset.append((token_id, cur_time + cur_relative_time))\n",
    "                elif cur_onset == 0:\n",
    "                    i = find_note(list_onset, token_id)\n",
    "                    if i >= 0:\n",
    "                        start = list_onset[i][1]\n",
    "                        end = cur_time + cur_relative_time\n",
    "                        if start < end:\n",
    "                            new_note = pm.Note(100, token_id, start, end)\n",
    "                            piano.notes.append(new_note)\n",
    "                        list_onset.pop(i)\n",
    "\n",
    "            elif token_name == \"onset\":\n",
    "                if tie_end:\n",
    "                    if token_id == 1:\n",
    "                        cur_onset = 1\n",
    "                    elif token_id == 0:\n",
    "                        cur_onset = 0\n",
    "            elif token_name == \"time\":\n",
    "                if tie_end:\n",
    "                    cur_relative_time = to_second(token_id)\n",
    "            elif token_name == \"endtie\":\n",
    "                tie_end = True\n",
    "                for note, start in list_onset:\n",
    "                    if note not in list_tie:\n",
    "                        if start < cur_time:\n",
    "                            new_note = pm.Note(100, note, start, cur_time)\n",
    "                            piano.notes.append(new_note)\n",
    "                        list_onset.remove((note, start))\n",
    "        cur_time += AUDIO_SEGMENT_SEC\n",
    "\n",
    "    midi_data.instruments.append(piano)\n",
    "    return midi_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We use MAESTRO v3.0.0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMTDatasetBase(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        flist_audio,\n",
    "        flist_midi,\n",
    "        sample_rate,\n",
    "        voc_dict,\n",
    "        apply_pedal=True,\n",
    "        whole_song=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.midi_filelist = flist_midi\n",
    "        self.audio_filelist = flist_audio\n",
    "        self.audio_metalist = [torchaudio.info(f) for f in flist_audio]\n",
    "        self.voc_dict = voc_dict\n",
    "        self.midi_list = [\n",
    "            MIDITokenExtractor(f, voc_dict, apply_pedal)\n",
    "            for f in tqdm.tqdm(self.midi_filelist, desc=\"load dataset\")\n",
    "        ]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.whole_song = whole_song\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_filelist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return a pair of (audio, tokens) for the given index.\n",
    "        On the training stage, return a random segment from the song.\n",
    "        On the test stage, return the audio and MIDI of the whole song.\n",
    "        \"\"\"\n",
    "        if not self.whole_song:\n",
    "            return self.getitem_segment(index)\n",
    "        else:\n",
    "            return self.getitem_wholesong(index)\n",
    "\n",
    "    def getitem_segment(self, index, start_pos=None):\n",
    "        metadata = self.audio_metalist[index]\n",
    "        num_frames = metadata.num_frames\n",
    "        sample_rate = metadata.sample_rate\n",
    "        duration_y = round(num_frames / float(sample_rate) * FRAME_PER_SEC)\n",
    "        midi_item = self.midi_list[index]\n",
    "        if start_pos is None:\n",
    "            segment_start = np.random.randint(duration_y - SEGMENT_N_FRAMES)\n",
    "        else:\n",
    "            segment_start = start_pos\n",
    "        segment_end = segment_start + SEGMENT_N_FRAMES\n",
    "        segment_start_sample = round(\n",
    "            segment_start * FRAME_STEP_SIZE_SEC * sample_rate\n",
    "        )\n",
    "\n",
    "        segment_tokens = midi_item.get_segment_tokens(segment_start, segment_end)\n",
    "        segment_tokens = torch.from_numpy(segment_tokens).long()\n",
    "        y_segment, _ = torchaudio.load(\n",
    "            self.audio_filelist[index],\n",
    "            frame_offset=segment_start_sample,\n",
    "            num_frames=round(AUDIO_SEGMENT_SEC * sample_rate),\n",
    "        )\n",
    "        y_segment = y_segment.mean(0)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            y_segment = torchaudio.functional.resample(\n",
    "                y_segment,\n",
    "                sample_rate,\n",
    "                self.sample_rate,\n",
    "                resampling_method=\"kaiser_window\",\n",
    "            )\n",
    "        return y_segment, segment_tokens\n",
    "\n",
    "    def getitem_wholesong(self, index):\n",
    "        y, sr = torchaudio.load(self.audio_filelist[index])\n",
    "        y = y.mean(0)\n",
    "        if sr != self.sample_rate:\n",
    "            y = torchaudio.functional.resample(\n",
    "                y, sr, self.sample_rate, \n",
    "                resampling_method=\"kaiser_window\"\n",
    "            )\n",
    "        midi = self.midi_list[index].pm\n",
    "        return y, midi\n",
    "\n",
    "    def collate_wholesong(self, batch):\n",
    "        batch_audio = torch.stack([b[0] for b in batch], dim=0)\n",
    "        midi = [b[1] for b in batch]\n",
    "        return batch_audio, midi\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        batch_audio = torch.stack([b[0] for b in batch], dim=0)\n",
    "        batch_tokens = [b[1] for b in batch]\n",
    "        batch_tokens_pad = torch.nn.utils.rnn.pad_sequence(\n",
    "            batch_tokens, batch_first=True, padding_value=self.voc_dict[\"pad\"]\n",
    "        )\n",
    "        return batch_audio, batch_tokens_pad\n",
    "    \n",
    "    \n",
    "class Maestro(AMTDatasetBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_files=-1,\n",
    "        sample_rate=44100,\n",
    "        split=\"test\",\n",
    "        apply_pedal=True,\n",
    "        whole_song=False,\n",
    "    ):\n",
    "        data_path = \"../../../Data2/maestro-v3.0.0/\"\n",
    "        df_metadata = pd.read_csv(os.path.join(data_path, \"maestro-v3.0.0.csv\"))\n",
    "        flist_audio = []\n",
    "        flist_midi = []\n",
    "        list_title = []\n",
    "        for row in range(len(df_metadata)):\n",
    "            if df_metadata[\"split\"][row] == split:\n",
    "                f_audio = os.path.join(data_path, df_metadata[\"audio_filename\"][row])\n",
    "                f_midi = os.path.join(data_path, df_metadata[\"midi_filename\"][row])\n",
    "                assert os.path.exists(f_audio) and os.path.exists(f_midi)\n",
    "                flist_audio.append(f_audio)\n",
    "                flist_midi.append(f_midi)\n",
    "                list_title.append(df_metadata[\"canonical_title\"][row])\n",
    "        if n_files > 0:\n",
    "            flist_audio = flist_audio[:n_files]\n",
    "            flist_midi = flist_midi[:n_files]\n",
    "        super().__init__(\n",
    "            flist_audio,\n",
    "            flist_midi,\n",
    "            sample_rate,\n",
    "            voc_dict=voc_single_track,\n",
    "            apply_pedal=apply_pedal,\n",
    "            whole_song=whole_song,\n",
    "        )\n",
    "        self.list_title = list_title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi(midi: pm.PrettyMIDI, program=0):\n",
    "    intervals = []\n",
    "    pitches = []\n",
    "    pm_notes = midi.instruments[program].notes\n",
    "    for note in pm_notes:\n",
    "        intervals.append((note.start, note.end))\n",
    "        pitches.append(note.pitch)\n",
    "\n",
    "    return np.array(intervals), np.array(pitches)\n",
    "\n",
    "\n",
    "def evaluate_midi(est_midi: pm.PrettyMIDI, ref_midi: pm.PrettyMIDI, program=0):\n",
    "    est_intervals, est_pitches = extract_midi(est_midi, program)\n",
    "    ref_intervals, ref_pitches = extract_midi(ref_midi, program)\n",
    "\n",
    "    dict_eval = mir_eval.transcription.evaluate(\n",
    "        ref_intervals, ref_pitches, est_intervals, est_pitches)\n",
    "\n",
    "    return dict_eval\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence transcriber\n",
    "\n",
    "The transformer model is built with the configuration of \"T5v1.1-small\" model released by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_into_segments(y: torch.Tensor, sr: int):\n",
    "    audio_segment_samples = round(AUDIO_SEGMENT_SEC * sr)\n",
    "    pad_size = audio_segment_samples - (y.shape[-1] % audio_segment_samples)\n",
    "    y = F.pad(y, (0, pad_size))\n",
    "    assert (y.shape[-1] % audio_segment_samples) == 0\n",
    "    n_chunks = y.shape[-1] // audio_segment_samples\n",
    "    y_segments = torch.chunk(y, chunks=n_chunks, dim=-1)\n",
    "    return torch.stack(y_segments, dim=0)\n",
    "\n",
    "\n",
    "def unpack_sequence(x: torch.Tensor, eos_id: int=1):\n",
    "    seqs = []\n",
    "    max_length = x.shape[-1]\n",
    "    for seq in x:\n",
    "        start_pos = 0\n",
    "        pos = 0\n",
    "        while (pos < max_length) and (seq[pos] != eos_id):\n",
    "            pos += 1\n",
    "        end_pos = pos+1\n",
    "        seqs.append(seq[start_pos:end_pos])\n",
    "    return seqs\n",
    "\n",
    "class LogMelspec(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, n_mels, hop_length):\n",
    "        super().__init__()\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            f_min=20.0,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"slaney\",\n",
    "            norm=\"slaney\",\n",
    "            power=1,\n",
    "        )\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        spec = self.melspec(x)\n",
    "        safe_spec = torch.clamp(spec, min=self.eps)\n",
    "        log_spec = torch.log(safe_spec)\n",
    "        return log_spec\n",
    "\n",
    "class Seq2SeqTranscriber(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, sample_rate: int, n_fft: int, hop_length: int, voc_dict: dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.infer_max_len = 200\n",
    "        self.voc_dict = voc_dict\n",
    "        self.n_voc_token = voc_dict[\"n_voc\"]\n",
    "        self.t5config = T5Config.from_pretrained(\"google/t5-v1_1-small\")\n",
    "        custom_configs = {\n",
    "            \"vocab_size\": self.n_voc_token,\n",
    "            \"pad_token_id\": voc_dict[\"pad\"],\n",
    "            \"d_model\": n_mels,\n",
    "        }\n",
    "\n",
    "        for k, v in custom_configs.items():\n",
    "            self.t5config.__setattr__(k, v)\n",
    "\n",
    "        self.transformer = T5ForConditionalGeneration(self.t5config)\n",
    "        self.melspec = LogMelspec(sample_rate, n_fft, n_mels, hop_length)\n",
    "        self.sr = sample_rate\n",
    "\n",
    "    def forward(self, wav, labels):\n",
    "        spec = self.melspec(wav).transpose(-1, -2)\n",
    "        outs = self.transformer.forward(\n",
    "            inputs_embeds=spec, return_dict=True, labels=labels\n",
    "        )\n",
    "        return outs\n",
    "\n",
    "    def infer(self, wav):\n",
    "        \"\"\"\n",
    "        Infer the transcription of a single audio file.\n",
    "        The input audio file is split into segments of 2 seconds\n",
    "        before passing to the transformer.\n",
    "        \"\"\"\n",
    "        wav_segs = split_audio_into_segments(wav, self.sr)\n",
    "        spec = self.melspec(wav_segs).transpose(-1, -2)\n",
    "        outs = self.transformer.generate(\n",
    "            inputs_embeds=spec,\n",
    "            max_length=self.infer_max_len,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=False,\n",
    "        )\n",
    "        return outs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitTranscriber(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transcriber_args: dict,\n",
    "        lr: float,\n",
    "        lr_decay: float = 1.0,\n",
    "        lr_decay_interval: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.voc_dict = voc_single_track\n",
    "        self.n_voc = self.voc_dict[\"n_voc\"]\n",
    "        self.transcriber = Seq2SeqTranscriber(\n",
    "            **transcriber_args, voc_dict=self.voc_dict\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_decay_interval = lr_decay_interval\n",
    "\n",
    "    def forward(self, y: torch.Tensor):\n",
    "        transcriber_infer = self.transcriber.infer(y)\n",
    "        return transcriber_infer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y, t = batch\n",
    "        tf_out = self.transcriber(y, t)\n",
    "        loss = tf_out.loss\n",
    "        t = t.detach()\n",
    "        mask = t != self.voc_dict[\"pad\"]\n",
    "        accr = (tf_out.logits.argmax(-1)[mask] == t[mask]).sum() / mask.sum()\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/accr\", accr)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        assert not self.transcriber.training\n",
    "        y, t = batch\n",
    "        tf_out = self.transcriber(y, t)\n",
    "        loss = tf_out.loss\n",
    "        t = t.detach()\n",
    "        mask = t != self.voc_dict[\"pad\"]\n",
    "        accr = (tf_out.logits.argmax(-1)[mask] == t[mask]).sum() / mask.sum()\n",
    "        self.log(\"vali/loss\", loss)\n",
    "        self.log(\"vali/accr\", accr)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y, ref_midi = batch\n",
    "        y = y[0]\n",
    "        ref_midi = ref_midi[0]\n",
    "        with torch.no_grad():\n",
    "            est_tokens = self.forward(y)\n",
    "            unpadded_tokens = unpack_sequence(est_tokens.cpu().numpy())\n",
    "            unpadded_tokens = [t[1:] for t in unpadded_tokens]\n",
    "            est_midi = token_seg_list_to_midi(unpadded_tokens)\n",
    "        dict_eval = evaluate_midi(est_midi, ref_midi)\n",
    "        dict_log = {}\n",
    "        for key in dict_eval:\n",
    "            dict_log[\"test/\" + key] = dict_eval[key]\n",
    "        self.log_dict(dict_log, batch_size=1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dset = Maestro(\n",
    "            sample_rate=SR,\n",
    "            split=\"train\",\n",
    "            n_files=20,\n",
    "        )\n",
    "        return data.DataLoader(\n",
    "            dataset=dset,\n",
    "            collate_fn=dset.collate_batch,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dset = Maestro(\n",
    "            sample_rate=SR,\n",
    "            split=\"test\",\n",
    "            whole_song=True,\n",
    "            n_files=10,\n",
    "        )\n",
    "        return data.DataLoader(\n",
    "            dataset=dset,\n",
    "            collate_fn=dset.collate_wholesong,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pioneerdj/anaconda3/envs/torch_music/lib/python3.10/site-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (512) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pioneerdj/anaconda3/envs/torch_music/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | transcriber | Seq2SeqTranscriber | 44.4 M\n",
      "---------------------------------------------------\n",
      "44.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.4 M    Total params\n",
      "177.645   Total estimated model params size (MB)\n",
      "load dataset: 100%|██████████| 20/20 [01:52<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s, loss=2.63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s, loss=2.63]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"n_mels\":512,\n",
    "    \"sample_rate\":16000,\n",
    "    \"n_fft\":1024,\n",
    "    \"hop_length\":256,\n",
    "}\n",
    "\n",
    "lightning_module = LitTranscriber(\n",
    "    transcriber_args=args,\n",
    "    lr=1e-4,\n",
    "    lr_decay=0.99\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=\"1,\",\n",
    "    max_epochs=1000,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "load dataset: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]\n",
      "/home/pioneerdj/anaconda3/envs/torch_music/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  20%|██        | 2/10 [00:04<00:16,  2.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pioneerdj/anaconda3/envs/torch_music/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('test/Average_Overlap_Ratio', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:38<00:00,  3.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">             Test metric              </span>┃<span style=\"font-weight: bold\">             DataLoader 0             </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/Average_Overlap_Ratio      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.44270918821881045          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/Average_Overlap_Ratio_no_offset </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.23869087929446692          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            test/F-measure            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.0004150962922722101         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/F-measure_no_offset       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.005259971134364605         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/Offset_F-measure         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.11814618110656738          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/Offset_Precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.1667836457490921          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/Offset_Recall          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.10361303389072418          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/Onset_F-measure         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.02616998553276062          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/Onset_Precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.03701071813702583          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/Onset_Recall           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.02266177348792553          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            test/Precision            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.00045526440953835845        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/Precision_no_offset       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.00696998555213213          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">             test/Recall              </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.0003843010345008224         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/Recall_no_offset         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.004659607540816069         </span>│\n",
       "└──────────────────────────────────────┴──────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m            Test metric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m            DataLoader 0            \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/Average_Overlap_Ratio     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.44270918821881045         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/Average_Overlap_Ratio_no_offset\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.23869087929446692         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           test/F-measure           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.0004150962922722101        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/F-measure_no_offset      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.005259971134364605        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/Offset_F-measure        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.11814618110656738         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/Offset_Precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.1667836457490921         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/Offset_Recall         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.10361303389072418         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/Onset_F-measure        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.02616998553276062         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/Onset_Precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.03701071813702583         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/Onset_Recall          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.02266177348792553         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           test/Precision           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.00045526440953835845       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/Precision_no_offset      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.00696998555213213         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m            test/Recall             \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.0003843010345008224        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/Recall_no_offset        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.004659607540816069        \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────────────────────────────┴──────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/Precision': 0.00045526440953835845,\n",
       "  'test/Recall': 0.0003843010345008224,\n",
       "  'test/F-measure': 0.0004150962922722101,\n",
       "  'test/Average_Overlap_Ratio': 0.44270918821881045,\n",
       "  'test/Precision_no_offset': 0.00696998555213213,\n",
       "  'test/Recall_no_offset': 0.004659607540816069,\n",
       "  'test/F-measure_no_offset': 0.005259971134364605,\n",
       "  'test/Average_Overlap_Ratio_no_offset': 0.23869087929446692,\n",
       "  'test/Onset_Precision': 0.03701071813702583,\n",
       "  'test/Onset_Recall': 0.02266177348792553,\n",
       "  'test/Onset_F-measure': 0.02616998553276062,\n",
       "  'test/Offset_Precision': 0.1667836457490921,\n",
       "  'test/Offset_Recall': 0.10361303389072418,\n",
       "  'test/Offset_F-measure': 0.11814618110656738}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The scores are low since the model is trained on a small dataset.\n",
    "\n",
    "trainer.test(lightning_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2b9369b0129de51f2fab34d1d8c136cdd90000b6b95ee076d5e25d27c5a753a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
